Welcome file
Welcome file
# Описание проекта

Проект был сделан для Национального медицинского исследовательского центра психиатрии и неврологии им. В. М. Бехтерева. Цель проекта создать инструмент для проведения исследований-опросов . На данный момент представленный продукт активно применяется исследовательским центром и получает новые применения в различных исследованиях.

# Структура проекта:

Проект можно подразделить на две части:
+ Сайт для регистрации участников исследования, ввода информации по исследованию (опрос )
+ Программный анализ данных и визуализация результата

Каждая часть проекта способна работать отдельно от другой и применяется независимо друг от друга. В  примере данные полученные с помощью сайта будут использованы для программного анализа

## Cайт:

Фреймворком для сайта был выбран Django. Питон-язык, которым я владею лучше всего, Django удобен в использование и универсален. Так что проект можно назвать full-python. База данных использованная в проекте - SqlLite3. Для создания динамических форм был использован формат JSON. Поскольку по-стандарту SqlLite3 не поддерживает этот формат, то для работы с базой данных следует заменить sqllte.dll в корневой папке python на файл прикрепленный к проекту.

### Главная страница:

Сайт - платформа для создания опросных форм администратором и прохождения опросов пользователем.

На сайте реализована систем аккаунтов, с регистрацией и  авторизацией пользователей. Система аккаунтов позволяет разделить пользователей на администрацию и на обычных пользователей. Главная страница сайта содержит поле для общей информации( это поле  - центр заполнил на свое усмотрение, поэтому в проекте сайта оно пустое), кнопок "Авторизация", "Регистрация", "Главная страница". Фактически главная страница служит буфером, чтобы не допустить пользователя к опросу, пока он не зарегистрирован . После регистрации пользователя перенаправляют на страницу авторизации, откуда он попадает на основную страницу исследований.

### Немного про страницу исследований:

Исследования, отображаемые в меню, создаются администратором ( при входе администратора на страницу отображается дополнительная кнопка "Панель вопросов")

Нажимая на нее, администратор попадает на панель создания/редактирования/удаления опросов.

С этой же страницы можно получить ссылку на сам опросник и скачать .csv файл с данными опроса.

Как только администратор создает опросник, на боковой панели страницы появляется его  кликабельное название, ведущие на сам опрос.

![страница исследования] https://github.com/Kew1710/pictures/blob/main/p2.png

### Страница опроса:

Опрос может состоять из 4 видов вопросов:

+ Численный ответ - шкала (максимальное и минимальное число установлены администратором)

+ Бинарный ответ(ползунок "Да-Нет")

+ Текстовый ответ с ограниченным количеством символов (короткий ответ в контексте вопроса). 

+ Текстовый ответ с неограниченным количество символов( важная часть для будущего анализа данных)

Количество вопросов как общее, так и для каждой категории неограниченно.

#### Пример оформления опроса и заполнения опроса:
![Пример оформления опроса] https://github.com/Kew1710/pictures/blob/main/p1.png

## Анализ данных:

Анализ данных был разбит на 3 основные части:

+ Первичная статистическая обработка и ее визуализация с помощью MatPlotLib

+ Тематический анализ текста на основе Gensim и Pyldavis и дополнительных библиотек

+ Статистическая обработка на основе программы SPSS

### Поговорим о каждой части отдельно:

#### Первичная статистическая обработка:

Первичная статистическая обработка представляет собой обработку табличных данных таких, как:

+ Распределение по регионам

+ Несколько ответов с динамикой изменения

+ Процентное распределение по конкретному вопросу

Визуализация выполнена с помощью MatPlotLib. В папке DataVisualMain содержаться табличные данные для примера (кстати, получены они в ходе первого применения проекта)

Пример визуализации:
![пример визуализации] https://github.com/Kew1710/pictures/blob/main/p3.png

#### Тематический анализ текста:

Вероятно, самая интересная часть работы, как для меня так и для людей, пользующихся продуктом. Идея простая, очень часто исследование требует от опрашиваемого развернутый ответ, например при исследование влияния конкретного препарата. Важно, чтобы сам пациент описал свое состояние , не пользуясь готовыми ответами , которые могут быть неточными или вообще заставить человека ответить не так, как он на самом деле думает. Но исследователю необходимо отделить информативную часть текста от информационного "шума", например:" Все нормально. Да вот только, поцарапался и не знаю чем лечить. Может йодом помазать, а может ....".

Упростит обработку обработку если автоматически разделять тексты по их смыслу, то есть по их ** ТЕМЕ **. Моя реализация позволяет не только получить данные о каждом тексте в табличном виде, но и увидеть визуализацию результата обработки данных.

##### Для тематического анализа текст следует сначала обработать:
+ Удалить лишние символы (знаки препинания, лишние пробелы) они создают "шум" при дальнейшем анализе.
+ Проверить орфографию написанного текста. Для этого воспользуемся YandexSpeller. Исправим опечатки и грамматические ошибки.
+ Удалить стоп-слова (слова не несущие смысл например: предлоги, союзы, местоимения, общие слова характерные для всех текстов. Например, для определенных медицинских текстов слово "пациент" не несет никакой смысловой нагрузки, поэтому его можно выкинуть). Для удаления стоп-слов я использовал nltk corpus, к которому я добавил свои стоп-слова по просьбе руководителя исследования. У корпуса большой объем слов на русском языке, что нам и нужно.

+ Лемматизировать текст. Лемма - начальная словарная форма слова. Лемматизация нужна, чтобы одинаковые по значению слова (лечебное-лечебный, шел-идти) могли быть восприняты машиной одинаково. Проблема лемматизации достаточно сложная. Мы не можем просто откинуть какую-то часть слова, по каким-то правилам прибавить другую и получить начальную форму слова. Для лемматизации текста на русском языке есть два основных морфологических анализатора:Pymorphy2 и pymystem3. Морфологические анализаторы работают на базе нейронных сетей. Эти анализаторы отличаются. Pymystem - это продукт Яндекса и распространяется он в виде "бинарника" от чего его время работы сильно увеличивается. Pymorphy распространяется по свободной лицензии и является открытой библиотекой. В их работе есть различия, Pymorhy и Pymystem по разному обрабатывают одинаковые слова, связанно это с тем, что pymystem обучается, в том числе на новостных сайтах, поэтому узко употребимые слова и кальки иностранных слов на русский воспринимаются им нормально, в отличие от pymorhy.Но нашему проекту не надо понимать, что чиллить - это чил, а Эрдоган - не Эрдогать, так что мы воспользуемся pymorhy, потому что она быстрее.

+ Токенизировать тексты. Фактически превратить каждый текст в набор токенов (слов и биграмм). Биграмма - это устойчивая языковая единица в данном контексте. Например, в моем тексте это были сочетания: "Медицинский персонал", " Средства защиты". Вместе такие слова образуют новые смысл, поэтому должны быть вынесены, как отдельные токены.

+ Создать словарь словарь и корпус токенов. Фактически корпус состоит из количества вхождений слова в текст и номера текста. Словарь же является списком уникальных слов во всех текстах.

##### Построение lda модели:

lda- вид тематической модели основной на латентном размещение Дирихле. Надо понимать, что lda не находит тексты наиболее близкие к заданным темам, она вообще не имеет заданных тем, мы можем задать только количество, которое модель попытается выделить . Для lda модели нужна Терм-документная матрица. Терм-документная матрица — это матрица которая имеет размер N *W ,где N — количество документов в корпусе, а W — размер словаря корпуса т.е. количество слов(уникальных) которые встречаются в нашем корпусе. В i-й строке, j-м столбце матрицы находится число — сколько раз в i-м тексте встретилось j-е слово.LDA строит, для данной Терм-документной матрицы и T заранее заданного числа тем — два распределения:

+ Распределение тем по текстам

+ Распределение слов по темам

Как определить тему? Выявить наиболее часто встречающиеся вместе слова. Действительно, слово "Блин" и слово "термоядерный" встречаются вместе реже, чем например "Блин" и "Варенье". Как определить какая тема у текста? Однозначно, сказать о чем текст не всегда удается, можно выявить совпадение текста с какой-то темой (совпадение частоты слов, например), таких тем может быть несколько, мы можем выделить доминирующую тему. Реализована модель  с помощью библиотеки gensim на основе машинного обучения. Обучать  модель мы будем на том же тексте, что и анализировать, чаще всего следует делать иначе, но найти похожий корпус текстов у нас не выйдет, так что сделаем так. Если вдаваться в детали, то настройка lda модели потребует подробного разбора математической теории кластеризации , поэтому обсудим только некоторые параметры настройки модели.
``` Python
lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,  
										  id2word=id2word,  
											 num_topics=5,  
										 random_state=100,  
											update_every=1,  
											chunksize=1000,  
												passes=3000,  
													 alpha=1,  
												  eta='auto',  
  
  per_word_topics=True)
 ```
+ num_topics-количество тем, которые алгоритм попытается выделить. О оптимальном количестве тем мы поговорим чуть позже.

+ random_state- фактически генератор случайных чисел, нужный нейронной сети для инициализации различных параметров

+ update_every- параметр, отвечающий за то, как часто будет обновляться наша модель. Выставим значение 1(после каждого текста), для наибольшей точности

+ chunksize-это количество документов, которые будут использоваться в каждом обучающем чанке

+ passes-общее количество проходов обучения модели

+ alpha-гиперпараметр, фактически априорное распределение Дирихле. Параметр отвечает за соотношение тема-документ, то есть плотность тем в документе.

+ Eta-гиперпараметр, параметр отвечает за плотность слов в теме.

Давайте поговорим о параметрах модели. Тема называется когерентной (согласованной), если термины, наиболее частые в данной теме, неслучайно часто совместно встречаются рядом в документах корпуса.

Перплексия — это метрика, используемая для оценки качества языковой модели. Говоря по-другому, ее сложности.

Я советую построить модель для разных конфигураций этих параметров, чтоб определить наилучшую модель по двум параметрам описанным выше. Эмпирически я пришел к 5 темам и alpha=0,7, я пробовал разные конфигурации и пришел к той,что видна на скриншоте . Вы можете оценить динамику изменения качества модели с помощью MatPlotLib или другой библиотеки для визуализации. 
##### Визуализация lda модели:
Пакет PylDavis позволяет создать интерактивную диаграмму.

Что означает диаграмма?
Каждый пузырь на левом графике представляет тему. Чем больше пузырь, тем больше распространена эта тема.
Хорошая тематическая модель будет иметь довольно большие непересекающиеся пузыри, разбросанные по всей диаграмме, а не сгруппированные в одном квадранте.
Модель со слишком большим количеством тем, как правило, имеет много перекрытий, пузырьки небольшого размера, сгруппированные в одной области диаграммы.
Если вы наведете курсор на один из пузырей то справа обновятся полосы графиков. Эти слова являются ключевыми словами, они формируют выбранную тему.
Визуализация с помощью пакета Pyldavis невероятно информативна и удобна в использование, но все-таки мы хотим определять к какой теме относиться каждый текст, а визуализация в этом нам не поможет. Надо представить информацию о доминирующей теме каждого текста  в табличном виде.
Воспользуемся следующим кодом:
``` Python
def format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=texts):  
  
    sent_topics_df = pd.DataFrame()  
  
    for i, row in enumerate(ldamodel[corpus]):  
        row = sorted(row[0], key=lambda x: (x[1]), reverse=True)  
  
        for j, (topic_num, prop_topic) in enumerate(row):  
            if j == 0:  
                wp = ldamodel.show_topic(topic_num)  
                topic_keywords = ", ".join([word for word, prop in wp])  
                sent_topics_df = sent_topics_df.append(  
                    pd.Series([int(topic_num), round(prop_topic, 4), topic_keywords]), ignore_index=True)  
            else:  
                break  
  sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']  
  
    contents = pd.Series(texts)  
    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)  
    return (sent_topics_df)  
  
  
df_topic_sents_keywords = format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=texts)  
  
df_dominant_topic = df_topic_sents_keywords.reset_index()  
df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']

writer = pd.ExcelWriter('data.xlsx', engine='xlsxwriter')  
df_dominant_topic.to_excel(writer, 'Sheet1')  
writer.save()
```
Вот и построили тематическую модель, визуализировали ее, провели анализ данных и сохранили результат в виде .xlsx таблицы. Тематический анализ очень интересная и сложная тема в машинном обучение, модель зависит от множества параметров и требует тонкой настройки. В моем проекте настройка была основана на моем эмпирическом опыте, и даже так мне удалось добиться высокой эффективности модели.

####  Статистическая обработка на основе программы SPSS

Для дальнейшей статистической обработки данные опроса выгружались в формате  Excel. Файлы загружались в программу SPSS, где проводился их статистический анализ.
Статистическая обработка данных включала методы описательной статистики (средние значения, стандартное отклонение, 95% доверительный интервал). Проверка на нормальность распределения осуществлена с помощью теста Колмогорова – Смирнова.  Для качественных признаков подсчитаны частоты и доли в процентах.  Для категориальных переменных были построены таблицы сопряженности, с  последующим применением χ 2 -Пирсона для оценки достоверности межгрупповых различий. Сравнение результатов для количественных показателей проводилось с помощью однофакторного дисперсионного анализ (one-way ANOVA), а также непараметрического критерия Манна-Уитни. Различия считались статистически значимыми при р < 0,05.
Результаты анализа прикреплены к проекту в виде xlsx таблицы.


Описание проекта
Проект был сделан для Национального медицинского исследовательского центра психиатрии и неврологии им. В. М. Бехтерева. Цель проекта создать инструмент для проведения исследований-опросов . На данный момент представленный продукт активно применяется исследовательским центром и получает новые применения в различных исследованиях.

Структура проекта:
Проект можно подразделить на две части:

Сайт для регистрации участников исследования, ввода информации по исследованию (опрос )
Программный анализ данных и визуализация результата
Каждая часть проекта способна работать отдельно от другой и применяется независимо друг от друга. В примере данные полученные с помощью сайта будут использованы для программного анализа

Cайт:
Фреймворком для сайта был выбран Django. Питон-язык, которым я владею лучше всего, Django удобен в использование и универсален. Так что проект можно назвать full-python. База данных использованная в проекте - SqlLite3. Для создания динамических форм был использован формат JSON. Поскольку по-стандарту SqlLite3 не поддерживает этот формат, то для работы с базой данных следует заменить sqllte.dll в корневой папке python на файл прикрепленный к проекту.

Главная страница:
Сайт - платформа для создания опросных форм администратором и прохождения опросов пользователем.

На сайте реализована систем аккаунтов, с регистрацией и авторизацией пользователей. Система аккаунтов позволяет разделить пользователей на администрацию и на обычных пользователей. Главная страница сайта содержит поле для общей информации( это поле - центр заполнил на свое усмотрение, поэтому в проекте сайта оно пустое), кнопок “Авторизация”, “Регистрация”, “Главная страница”. Фактически главная страница служит буфером, чтобы не допустить пользователя к опросу, пока он не зарегистрирован . После регистрации пользователя перенаправляют на страницу авторизации, откуда он попадает на основную страницу исследований.

Немного про страницу исследований:
Исследования, отображаемые в меню, создаются администратором ( при входе администратора на страницу отображается дополнительная кнопка “Панель вопросов”)

Нажимая на нее, администратор попадает на панель создания/редактирования/удаления опросов.

С этой же страницы можно получить ссылку на сам опросник и скачать .csv файл с данными опроса.

Как только администратор создает опросник, на боковой панели страницы появляется его кликабельное название, ведущие на сам опрос.

Страница опроса:
Опрос может состоять из 4 видов вопросов:

Численный ответ - шкала (максимальное и минимальное число установлены администратором)

Бинарный ответ(ползунок “Да-Нет”)

Текстовый ответ с ограниченным количеством символов (короткий ответ в контексте вопроса).

Текстовый ответ с неограниченным количество символов( важная часть для будущего анализа данных)

Количество вопросов как общее, так и для каждой категории неограниченно.

Пример оформления опроса и заполнения опроса:
Анализ данных:
Анализ данных был разбит на 3 основные части:

Первичная статистическая обработка и ее визуализация с помощью MatPlotLib

Тематический анализ текста на основе Gensim и Pyldavis и дополнительных библиотек

Статистическая обработка на основе программы SPSS

Поговорим о каждой части отдельно:
Первичная статистическая обработка:
Первичная статистическая обработка представляет собой обработку табличных данных таких, как:

Распределение по регионам

Несколько ответов с динамикой изменения

Процентное распределение по конкретному вопросу

Визуализация выполнена с помощью MatPlotLib. В папке DataVisualMain содержаться табличные данные для примера (кстати, получены они в ходе первого применения проекта)

Пример визуализации:

Тематический анализ текста:
Вероятно, самая интересная часть работы, как для меня так и для людей, пользующихся продуктом. Идея простая, очень часто исследование требует от опрашиваемого развернутый ответ, например при исследование влияния конкретного препарата. Важно, чтобы сам пациент описал свое состояние , не пользуясь готовыми ответами , которые могут быть неточными или вообще заставить человека ответить не так, как он на самом деле думает. Но исследователю необходимо отделить информативную часть текста от информационного “шума”, например:" Все нормально. Да вот только, поцарапался и не знаю чем лечить. Может йодом помазать, а может …".

Упростит обработку обработку если автоматически разделять тексты по их смыслу, то есть по их ** ТЕМЕ **. Моя реализация позволяет не только получить данные о каждом тексте в табличном виде, но и увидеть визуализацию результата обработки данных.

Для тематического анализа текст следует сначала обработать:
Удалить лишние символы (знаки препинания, лишние пробелы) они создают “шум” при дальнейшем анализе.

Проверить орфографию написанного текста. Для этого воспользуемся YandexSpeller. Исправим опечатки и грамматические ошибки.

Удалить стоп-слова (слова не несущие смысл например: предлоги, союзы, местоимения, общие слова характерные для всех текстов. Например, для определенных медицинских текстов слово “пациент” не несет никакой смысловой нагрузки, поэтому его можно выкинуть). Для удаления стоп-слов я использовал nltk corpus, к которому я добавил свои стоп-слова по просьбе руководителя исследования. У корпуса большой объем слов на русском языке, что нам и нужно.

Лемматизировать текст. Лемма - начальная словарная форма слова. Лемматизация нужна, чтобы одинаковые по значению слова (лечебное-лечебный, шел-идти) могли быть восприняты машиной одинаково. Проблема лемматизации достаточно сложная. Мы не можем просто откинуть какую-то часть слова, по каким-то правилам прибавить другую и получить начальную форму слова. Для лемматизации текста на русском языке есть два основных морфологических анализатора:Pymorphy2 и pymystem3. Морфологические анализаторы работают на базе нейронных сетей. Эти анализаторы отличаются. Pymystem - это продукт Яндекса и распространяется он в виде “бинарника” от чего его время работы сильно увеличивается. Pymorphy распространяется по свободной лицензии и является открытой библиотекой. В их работе есть различия, Pymorhy и Pymystem по разному обрабатывают одинаковые слова, связанно это с тем, что pymystem обучается, в том числе на новостных сайтах, поэтому узко употребимые слова и кальки иностранных слов на русский воспринимаются им нормально, в отличие от pymorhy.Но нашему проекту не надо понимать, что чиллить - это чил, а Эрдоган - не Эрдогать, так что мы воспользуемся pymorhy, потому что она быстрее.

Токенизировать тексты. Фактически превратить каждый текст в набор токенов (слов и биграмм). Биграмма - это устойчивая языковая единица в данном контексте. Например, в моем тексте это были сочетания: “Медицинский персонал”, " Средства защиты". Вместе такие слова образуют новые смысл, поэтому должны быть вынесены, как отдельные токены.

Создать словарь словарь и корпус токенов. Фактически корпус состоит из количества вхождений слова в текст и номера текста. Словарь же является списком уникальных слов во всех текстах.

Построение lda модели:
lda- вид тематической модели основной на латентном размещение Дирихле. Надо понимать, что lda не находит тексты наиболее близкие к заданным темам, она вообще не имеет заданных тем, мы можем задать только количество, которое модель попытается выделить . Для lda модели нужна Терм-документная матрица. Терм-документная матрица — это матрица которая имеет размер N *W ,где N — количество документов в корпусе, а W — размер словаря корпуса т.е. количество слов(уникальных) которые встречаются в нашем корпусе. В i-й строке, j-м столбце матрицы находится число — сколько раз в i-м тексте встретилось j-е слово.LDA строит, для данной Терм-документной матрицы и T заранее заданного числа тем — два распределения:

Распределение тем по текстам

Распределение слов по темам

Как определить тему? Выявить наиболее часто встречающиеся вместе слова. Действительно, слово “Блин” и слово “термоядерный” встречаются вместе реже, чем например “Блин” и “Варенье”. Как определить какая тема у текста? Однозначно, сказать о чем текст не всегда удается, можно выявить совпадение текста с какой-то темой (совпадение частоты слов, например), таких тем может быть несколько, мы можем выделить доминирующую тему. Реализована модель с помощью библиотеки gensim на основе машинного обучения. Обучать модель мы будем на том же тексте, что и анализировать, чаще всего следует делать иначе, но найти похожий корпус текстов у нас не выйдет, так что сделаем так. Если вдаваться в детали, то настройка lda модели потребует подробного разбора математической теории кластеризации , поэтому обсудим только некоторые параметры настройки модели.

lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,  
										  id2word=id2word,  
											 num_topics=5,  
										 random_state=100,  
											update_every=1,  
											chunksize=1000,  
												passes=3000,  
													 alpha=1,  
												  eta='auto',  
  
  per_word_topics=True)
num_topics-количество тем, которые алгоритм попытается выделить. О оптимальном количестве тем мы поговорим чуть позже.

random_state- фактически генератор случайных чисел, нужный нейронной сети для инициализации различных параметров

update_every- параметр, отвечающий за то, как часто будет обновляться наша модель. Выставим значение 1(после каждого текста), для наибольшей точности

chunksize-это количество документов, которые будут использоваться в каждом обучающем чанке

passes-общее количество проходов обучения модели

alpha-гиперпараметр, фактически априорное распределение Дирихле. Параметр отвечает за соотношение тема-документ, то есть плотность тем в документе.

Eta-гиперпараметр, параметр отвечает за плотность слов в теме.

Давайте поговорим о параметрах модели. Тема называется когерентной (согласованной), если термины, наиболее частые в данной теме, неслучайно часто совместно встречаются рядом в документах корпуса.

Перплексия — это метрика, используемая для оценки качества языковой модели. Говоря по-другому, ее сложности.

Я советую построить модель для разных конфигураций этих параметров, чтоб определить наилучшую модель по двум параметрам описанным выше. Эмпирически я пришел к 5 темам и alpha=0,7, я пробовал разные конфигурации и пришел к той,что видна на скриншоте . Вы можете оценить динамику изменения качества модели с помощью MatPlotLib или другой библиотеки для визуализации.

Визуализация lda модели:
Пакет PylDavis позволяет создать интерактивную диаграмму.

Что означает диаграмма?
Каждый пузырь на левом графике представляет тему. Чем больше пузырь, тем больше распространена эта тема.
Хорошая тематическая модель будет иметь довольно большие непересекающиеся пузыри, разбросанные по всей диаграмме, а не сгруппированные в одном квадранте.
Модель со слишком большим количеством тем, как правило, имеет много перекрытий, пузырьки небольшого размера, сгруппированные в одной области диаграммы.
Если вы наведете курсор на один из пузырей то справа обновятся полосы графиков. Эти слова являются ключевыми словами, они формируют выбранную тему.
Визуализация с помощью пакета Pyldavis невероятно информативна и удобна в использование, но все-таки мы хотим определять к какой теме относиться каждый текст, а визуализация в этом нам не поможет. Надо представить информацию о доминирующей теме каждого текста в табличном виде.
Воспользуемся следующим кодом:

def format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=texts):  
  
    sent_topics_df = pd.DataFrame()  
  
    for i, row in enumerate(ldamodel[corpus]):  
        row = sorted(row[0], key=lambda x: (x[1]), reverse=True)  
  
        for j, (topic_num, prop_topic) in enumerate(row):  
            if j == 0:  
                wp = ldamodel.show_topic(topic_num)  
                topic_keywords = ", ".join([word for word, prop in wp])  
                sent_topics_df = sent_topics_df.append(  
                    pd.Series([int(topic_num), round(prop_topic, 4), topic_keywords]), ignore_index=True)  
            else:  
                break  
  sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']  
  
    contents = pd.Series(texts)  
    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)  
    return (sent_topics_df)  
  
  
df_topic_sents_keywords = format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=texts)  
  
df_dominant_topic = df_topic_sents_keywords.reset_index()  
df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']

writer = pd.ExcelWriter('data.xlsx', engine='xlsxwriter')  
df_dominant_topic.to_excel(writer, 'Sheet1')  
writer.save()
Вот и построили тематическую модель, визуализировали ее, провели анализ данных и сохранили результат в виде .xlsx таблицы. Тематический анализ очень интересная и сложная тема в машинном обучение, модель зависит от множества параметров и требует тонкой настройки. В моем проекте настройка была основана на моем эмпирическом опыте, и даже так мне удалось добиться высокой эффективности модели.

Статистическая обработка на основе программы SPSS
Для дальнейшей статистической обработки данные опроса выгружались в формате Excel. Файлы загружались в программу SPSS, где проводился их статистический анализ.
Статистическая обработка данных включала методы описательной статистики (средние значения, стандартное отклонение, 95% доверительный интервал). Проверка на нормальность распределения осуществлена с помощью теста Колмогорова – Смирнова. Для качественных признаков подсчитаны частоты и доли в процентах. Для категориальных переменных были построены таблицы сопряженности, с последующим применением χ 2 -Пирсона для оценки достоверности межгрупповых различий. Сравнение результатов для количественных показателей проводилось с помощью однофакторного дисперсионного анализ (one-way ANOVA), а также непараметрического критерия Манна-Уитни. Различия считались статистически значимыми при р < 0,05.
Результаты анализа прикреплены к проекту в виде xlsx таблицы.

Markdown 14159 bytes 1776 words 183 lines Ln 175, Col 391HTML 11876 characters 1728 words 109 paragraphs
